{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1e4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2bd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import Vocabulary, PolEngDS, get_loader\n",
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591b2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    epochs:int = 20\n",
    "    learning_rate:float = 3e-4\n",
    "    batch_size:int = 4\n",
    "    limit:int = 100000\n",
    "    max_length:int = 50\n",
    "    embed_size:int = 256\n",
    "    num_layers:int = 3\n",
    "    heads:int = 8\n",
    "    forward_expansion:int = 4\n",
    "    dropout:int = 0.15\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c635ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeEnDS(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = list(Multi30k('./data_exp/multi30k-dataset/task1')[0])\n",
    "        \n",
    "        self.preprocessing()\n",
    "    def __getitem__(self, index):\n",
    "        de, en = [text.split() for text in self.data.iloc[index].values]\n",
    "\n",
    "        de = torch.IntTensor([self.vocab_de['<sos>'], *[self.vocab_de[word] for word in de], self.vocab_de['<eos>']])\n",
    "        en = torch.IntTensor([self.vocab_en['<sos>'], *[self.vocab_en[word] for word in en], self.vocab_en['<eos>']])\n",
    "\n",
    "        return de, en \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        preprocessed_data = {\n",
    "            'deutsch': [],\n",
    "            'english': []\n",
    "        }\n",
    "        \n",
    "        for de, en in self.data:\n",
    "            preprocessed_data['deutsch'].append(self._text_prep(de))\n",
    "            preprocessed_data['english'].append(self._text_prep(en))\n",
    "        \n",
    "        self.data = pd.DataFrame(preprocessed_data)\n",
    "        \n",
    "        self.vocab_de = Vocabulary(self._flat_list(self.data['deutsch']))\n",
    "        self.vocab_en = Vocabulary(self._flat_list(self.data['english']))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _text_prep(text):\n",
    "        #remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.strip().lower()\n",
    "        text.split('/n')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def _flat_list(data):\n",
    "        data = [text.split() for text in data]\n",
    "        return list(itertools.chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55751ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuba_\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "train_data = DeEnDS()\n",
    "\n",
    "vocab_de = train_data.vocab_de\n",
    "vocab_en = train_data.vocab_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccaf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(batch, padding_de=1, padding_en=1):\n",
    "    de, en = [], []\n",
    "\n",
    "    for i, (de_text, en_text) in enumerate(batch):\n",
    "        de.append(de_text)\n",
    "        en.append(en_text)\n",
    "\n",
    "    de = pad_sequence(de, batch_first=True, padding_value=padding_de)\n",
    "    en = pad_sequence(en, batch_first=True, padding_value=padding_en)\n",
    "\n",
    "    return de, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21872bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=pad_seq, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e10f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797990d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    src_vocab_size=len(vocab_de),\n",
    "    trg_vocab_size=len(vocab_en),\n",
    "    src_pad_idx=vocab_de['<pad>'],\n",
    "    trg_pad_idx=vocab_en['<pad>'],\n",
    "    embed_size=config.embed_size,\n",
    "    num_layers=config.num_layers,\n",
    "    heads=config.heads,\n",
    "    forward_expansion=config.forward_expansion,\n",
    "    dropout=config.dropout,\n",
    "    max_length=config.max_length,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b781399",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en['<pad>'])\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f0aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, epoch, device=device):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    t_batch = t0\n",
    "    \n",
    "    for batch_idx, (src, trg) in enumerate(loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        scores = model(src, trg[:, :-1])\n",
    "\n",
    "        loss = criterion(\n",
    "            scores.reshape(-1, scores.shape[2]), \n",
    "            trg[:, 1:].reshape(-1).type(torch.long)\n",
    "        )\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1) % int(len(loader) / 5) == 0:\n",
    "            print('Epoch: {epoch}, batch: {batch_idx}/{no_batches}, loss: {loss:.3f}, time: {t:.2f}'.format(\n",
    "                epoch=epoch+1,\n",
    "                batch_idx=batch_idx,\n",
    "                no_batches=len(loader),\n",
    "                loss=sum(losses)/len(losses),\n",
    "                t=time.time()-t_batch\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            t_batch = time.time()\n",
    "        \n",
    "    loss = sum(losses) / len(losses)\n",
    "    \n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    print('Epoch: {epoch}, loss: {loss:.3f}, time: {t:.2f}'.format(\n",
    "        epoch=epoch+1, \n",
    "        loss=loss, \n",
    "        t=time.time()-t0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4fb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    l = train_epoch(model, train_loader, epoch)\n",
    "    loss.append(l)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_sd': model.state_dict(),\n",
    "        'optimizer_sd': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "       }, f'./models/checkpoint-{epoch}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleng",
   "language": "python",
   "name": "poleng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
